# chat_interface.py - Centrum Dowodzenia Agentem v2.4 (Poprawka BÅ‚Ä™du API)
import os
import json
import sqlite3
import chromadb
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime

# Åadowanie konfiguracji agenta
load_dotenv()

def print_help():
    """WyÅ›wietla bardziej przyjaznÄ… i rozbudowanÄ… pomoc."""
    print("\n--- Centrum Dowodzenia Agentem ---")
    print("Rozmawiaj z agentem uÅ¼ywajÄ…c naturalnego jÄ™zyka. On zrozumie Twoje intencje.")
    print("\nPrzykÅ‚adowe polecenia, ktÃ³re moÅ¼esz wydaÄ‡:")
    print('  "jaki jest twÃ³j status?" lub "pokaÅ¼ raport"')
    print('  "co wiesz na temat projektu Monad?"')
    print('  "zapamiÄ™taj: od teraz skupiaj siÄ™ bardziej na analizie danych on-chain." (dodaje dyrektywÄ™)')
    print('  "zasymuluj posta na temat obecnej kondycji rynku."')
    print('  "ktÃ³re tematy dziaÅ‚ajÄ… najlepiej?" (analiza wydajnoÅ›ci)')
    print('  "pokaÅ¼ mi wszystkie moje dyrektywy."')
    print("\n  'pomoc' - wyÅ›wietla tÄ™ wiadomoÅ›Ä‡")
    print("  'wyjdÅº' - koÅ„czy sesjÄ™")
    print("----------------------------------\n")

class AgentInterface:
    """
    Zaawansowany interfejs do interakcji z pamiÄ™ciÄ…, stanem i procesami myÅ›lowymi agenta.
    """
    def __init__(self):
        print("Inicjalizacja Centrum Dowodzenia...")
        try:
            self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            db_client = chromadb.PersistentClient(path="agent_memory_db")
            self.vector_memory = db_client.get_or_create_collection(name="agent_memory")
            self.conn = sqlite3.connect("agent_state.db")
            self.cursor = self.conn.cursor()
            prompt_template = os.getenv('PROMPT_TEMPLATE', "You are an AI agent.")
            self.persona_primer = prompt_template.split('.')[0].strip() + "."
            print("âœ… Systemy online. Witaj w Centrum Dowodzenia.")
        except Exception as e:
            print(f"âŒ Krytyczny bÅ‚Ä…d podczas inicjalizacji: {e}")
            raise

    def interpret_command(self, user_input):
        """UÅ¼ywa LLM do zrozumienia intencji uÅ¼ytkownika i wybrania odpowiedniego narzÄ™dzia."""
        json_example = '{"tool": "nazwa_narzedzia", "args": {"argument": "wartosc"}}'
        
        # --- ZAKTUALIZOWANY SZABLON ---
        prompt_template = """
        JesteÅ› inteligentnym routerem poleceÅ„ dla interfejsu agenta AI. Twoim zadaniem jest przeksztaÅ‚cenie zapytania uÅ¼ytkownika w wywoÅ‚anie jednego z dostÄ™pnych narzÄ™dzi w formacie JSON.

        DostÄ™pne narzÄ™dzia:
        1. `generate_strategy_report`: OgÃ³lny status i ostatnie akcje.
        2. `synthesize_answer_from_memory`: Gdy uÅ¼ytkownik pyta o wiedzÄ™ na konkretny temat.
        3. `add_directive`: Gdy uÅ¼ytkownik chce dodaÄ‡ nowÄ…, trwaÅ‚Ä… instrukcjÄ™.
        4. `simulate_post_generation`: Symulacja posta na dany temat.
        5. `analyze_topic_performance`: Analiza, ktÃ³re tematy postÃ³w dziaÅ‚ajÄ… najlepiej.
        6. `list_memory_by_type`: Lista wspomnieÅ„ danego typu (np. dyrektywy).
        7. `list_vetted_partners`: Prosta lista zweryfikowanych partnerÃ³w i ich ocen. (np. "pokaÅ¼ zweryfikowanych partnerÃ³w")
        8. `analyze_partner_funnel`: GÅ‚Ä™boka, analityczna odpowiedÅº na temat strategii partnerskiej. (np. "jakie mamy plany wobec partnerÃ³w?", "przeanalizuj strategiÄ™ sieciowÄ…")
        9. `general_conversation`: JeÅ›li zapytanie jest ogÃ³lnÄ… rozmowÄ….
        
        Przeanalizuj poniÅ¼sze zapytanie i zwrÃ³Ä‡ TYLKO obiekt JSON w formacie: {json_example}
        Zapytanie uÅ¼ytkownika: "{user_input}"
        """
        final_prompt = prompt_template.format(json_example=json_example, user_input=user_input)
        try:
            response = self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "system", "content": final_prompt}],
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"BÅ‚Ä…d w `interpret_command`: {e}")
            return {"tool": "error", "args": {"message": str(e)}}

    def synthesize_answer_from_memory(self, topic: str, n_results=5):
        """Przeszukuje pamiÄ™Ä‡ i syntezuje znalezione informacje w spÃ³jnÄ… odpowiedÅº."""
        print(f"ğŸ§  PrzeszukujÄ™ pamiÄ™Ä‡ pod kÄ…tem: '{topic}'...")
        try:
            response = self.openai_client.embeddings.create(input=[topic], model="text-embedding-3-small")
            results = self.vector_memory.query(query_embeddings=[response.data[0].embedding], n_results=n_results)
            documents = results.get('documents', [[]])[0]
            if not documents: return "Nie znalazÅ‚em w mojej pamiÄ™ci Å¼adnych informacji na ten temat."

            formatted_documents = "\n- ".join(documents)
            synthesis_prompt = f"""
            {self.persona_primer}
            Twoim zadaniem jest odpowiedzieÄ‡ na pytanie uÅ¼ytkownika, bazujÄ…c na fragmentach Twojej wÅ‚asnej pamiÄ™ci. 
            Przeanalizuj poniÅ¼sze dane i stwÃ³rz z nich spÃ³jnÄ…, zwiÄ™zÅ‚Ä… odpowiedÅº w pierwszej osobie ("Moja analiza wskazuje...", "Z moich obserwacji wynika..."). Nie cytuj fragmentÃ³w dosÅ‚ownie, ale zinterpretuj je.

            Pytanie uÅ¼ytkownika: "{topic}"
            Fragmenty z Twojej pamiÄ™ci do analizy:
            - {formatted_documents}

            Twoja syntetyczna odpowiedÅº:
            """
            print("ğŸ¤– SyntezujÄ™ odpowiedÅº...")
            response = self.openai_client.chat.completions.create(model="gpt-4-turbo", messages=[{"role": "system", "content": synthesis_prompt}])
            return response.choices[0].message.content
        except Exception as e: return f"BÅ‚Ä…d podczas syntezy odpowiedzi: {e}"

    def generate_strategy_report(self):
        """Generuje kompleksowy raport o stanie i strategii agenta."""
        print("ğŸ“Š Generowanie raportu strategicznego...")
        report = "--- RAPORT STRATEGICZNY AGENTA ---\n"
        
        try:
            # CzÄ™Å›Ä‡ z bazy SQLite (bez zmian)
            self.cursor.execute("SELECT timestamp, action_name, target FROM action_log ORDER BY timestamp DESC LIMIT 5")
            actions = self.cursor.fetchall()
            report += "\n[Ostatnie 5 Akcji]\n" + ('\n'.join([f"- {ts[:16]}: {name} (Cel: {target})" for ts, name, target in actions]) if actions else "- Brak zarejestrowanych akcji.\n")
            
            # --- POPRAWIONA CZÄ˜ÅšÄ† Z BAZY WEKTOROWEJ ---
            # Krok 1: StwÃ³rz embedding dla zapytania uÅ¼ywajÄ…c tego samego modelu co agent (OpenAI)
            query_text = "strategic insights and user directives"
            response = self.openai_client.embeddings.create(input=[query_text], model="text-embedding-3-small")
            query_embedding = response.data[0].embedding
            
            # Krok 2: Przeszukaj bazÄ™ uÅ¼ywajÄ…c stworzonego embeddingu (query_embeddings)
            results = self.vector_memory.query(
                query_embeddings=[query_embedding],
                n_results=4,
                where={"$or": [{"type": "insight"}, {"type": "user_directive"}]}
            )
            
            memories = results.get('documents', [[]])[0]
            report += "\n[Kluczowe MyÅ›li KierujÄ…ce (PamiÄ™Ä‡)]\n" + ('\n'.join([f"- {mem}" for mem in memories]) if memories else "- Brak kluczowych myÅ›li w pamiÄ™ci.\n")

            report += "\n--- KONIEC RAPORTU ---"
            return report
            
        except Exception as e:
            # Zwracamy bardziej szczegÃ³Å‚owy bÅ‚Ä…d, jeÅ›li coÅ› pÃ³jdzie nie tak
            return f"BÅ‚Ä…d podczas generowania raportu: {e}"

    def add_directive(self, directive: str):
        """Dodaje nowÄ… dyrektywÄ™ od uÅ¼ytkownika do pamiÄ™ci wektorowej."""
        print(f"âœï¸ ZapisujÄ™ nowÄ… dyrektywÄ™: '{directive}'")
        try:
            response = self.openai_client.embeddings.create(input=[directive], model="text-embedding-3-small")
            self.vector_memory.add(
                embeddings=[response.data[0].embedding],
                documents=[directive],
                metadatas=[{"type": "user_directive"}],
                ids=[f"directive_{int(datetime.now().timestamp())}"]
            )
            return "âœ… Dyrektywa zostaÅ‚a zapisana. Agent uwzglÄ™dni jÄ… w przyszÅ‚ych dziaÅ‚aniach."
        except Exception as e: return f"âŒ Nie udaÅ‚o siÄ™ zapisaÄ‡ dyrektywy: {e}"

    def simulate_post_generation(self, topic: str):
        """Symuluje proces generowania tweeta na dany temat."""
        print(f"ğŸ’¡ SymulujÄ™ proces myÅ›lowy dla tematu: '{topic}'...")
        try:
            response = self.openai_client.embeddings.create(input=[f"strategic insights, directives, and past posts about {topic}"], model="text-embedding-3-small")
            results = self.vector_memory.query(query_embeddings=[response.data[0].embedding], n_results=4)
            context_docs = results.get('documents', [[]])[0]
            context_summary = "\n- ".join(context_docs) if context_docs else "Brak specyficznego kontekstu w pamiÄ™ci."
            
            prompt_template_env = os.getenv('PROMPT_TEMPLATE')
            final_prompt = prompt_template_env.format(observed_subject=topic, successful_examples=context_summary)
            
            print("ğŸ¤– GenerujÄ™ symulowany post...")
            response = self.openai_client.chat.completions.create(model=os.getenv("CREATION_MODEL", "gpt-4-turbo"), messages=[{"role": "user", "content": final_prompt}])
            content = response.choices[0].message.content.strip().strip('"')
            return f"--- WYNIK SYMULACJI ---\nTemat: {topic}\nWygenerowany post: \"{content}\"\n-------------------------"
        except Exception as e: return f"BÅ‚Ä…d podczas symulacji: {e}"

    def analyze_topic_performance(self):
        """Analizuje wydajnoÅ›Ä‡ tematÃ³w na podstawie danych z bazy SQLite."""
        print("ğŸ“ˆ AnalizujÄ™ wydajnoÅ›Ä‡ tematÃ³w...")
        try:
            self.cursor.execute("SELECT subject, COUNT(tweet_id), AVG(likes) FROM observations WHERE likes IS NOT NULL GROUP BY subject ORDER BY AVG(likes) DESC")
            data = self.cursor.fetchall()
            if not data: return "Brak wystarczajÄ…cych danych do analizy."
            
            report = "--- ANALIZA WYDAJNOÅšCI TEMATÃ“W ---\n"
            report += f"{'Temat':<30} | {'Liczba PostÃ³w':<15} | {'Åšr. PolubieÅ„':<15}\n" + "-"*65 + "\n"
            report += '\n'.join([f"{s:<30} | {c:<15} | {f'{a:.2f}':<15}" for s, c, a in data])
            return report
        except Exception as e: return f"BÅ‚Ä…d podczas analizy: {e}"

    def list_memory_by_type(self, memory_type: str):
        """WyÅ›wietla listÄ™ wspomnieÅ„ danego typu."""
        print(f"ğŸ“‹ PrzeglÄ…dam pamiÄ™Ä‡ w poszukiwaniu typu: '{memory_type}'...")
        try:
            results = self.vector_memory.get(where={"type": memory_type}, limit=10)
            documents = results.get('documents', [])
            if not documents: return f"Nie znaleziono w pamiÄ™ci Å¼adnych wpisÃ³w typu '{memory_type}'."
            return f"--- Wspomnienia typu: {memory_type} ---\n" + '\n'.join([f"- {doc}" for doc in documents])
        except Exception as e: return f"BÅ‚Ä…d podczas przeglÄ…dania pamiÄ™ci: {e}"
    def list_vetted_partners(self):
        """WyÅ›wietla sformatowanÄ… listÄ™ wszystkich zweryfikowanych, wartoÅ›ciowych partnerÃ³w."""
        print("ğŸ“‹ Pobieram listÄ™ zweryfikowanych partnerÃ³w...")
        try:
            self.cursor.execute("""
                SELECT screen_name, relevance_score, activity_score, legitimacy_score, llm_summary 
                FROM potential_partners 
                WHERE status='vetted' 
                ORDER BY (relevance_score + activity_score + legitimacy_score) DESC
            """)
            partners = self.cursor.fetchall()
            if not partners:
                return "Brak zweryfikowanych partnerÃ³w w bazie danych."
            
            report = "--- ZWERYFIKOWANI PARTNERZY (Ranking wg Oceny) ---\n"
            report += f"{'Profil':<20} | {'R/A/L':<10} | {'Podsumowanie AI'}\n"
            report += "-"*80 + "\n"
            for name, r, a, l, summary in partners:
                scores = f"{r}/{a}/{l}"
                report += f"{name:<20} | {scores:<10} | {summary}\n"
            return report
        except Exception as e:
            return f"BÅ‚Ä…d podczas pobierania listy partnerÃ³w: {e}"

    def analyze_partner_funnel(self):
        """Generuje kompleksowÄ…, analitycznÄ… odpowiedÅº na temat strategii partnerskiej."""
        print("ğŸ“Š AnalizujÄ™ lejek partnerski... To moÅ¼e chwilÄ™ potrwaÄ‡.")
        try:
            # 1. Statystyki ogÃ³lne lejka
            self.cursor.execute("SELECT status, COUNT(*) FROM potential_partners GROUP BY status")
            stats = self.cursor.fetchall()
            funnel_summary = "Statystyki Lejka Partnerskiego:\n" + "\n".join([f"- {status.capitalize()}: {count}" for status, count in stats])

            # 2. Pobierz dossier 3 najlepszych zweryfikowanych partnerÃ³w
            self.cursor.execute("""
                SELECT screen_name, relevance_score, activity_score, legitimacy_score, llm_summary 
                FROM potential_partners 
                WHERE status='vetted' 
                ORDER BY (relevance_score + activity_score + legitimacy_score) DESC LIMIT 3
            """)
            top_partners = self.cursor.fetchall()

            if not top_partners:
                return funnel_summary + "\n\nBrak zweryfikowanych partnerÃ³w do szczegÃ³Å‚owej analizy."

            dossiers = []
            for name, r, a, l, summary in top_partners:
                # 3. Dla kaÅ¼dego partnera, znajdÅº historiÄ™ interakcji
                self.cursor.execute("""
                    SELECT action_name, timestamp FROM action_log 
                    WHERE target = ? ORDER BY timestamp DESC LIMIT 5
                """, (name,))
                interactions = self.cursor.fetchall()
                
                interaction_history = "Brak zarejestrowanych interakcji."
                if interactions:
                    interaction_history = "Ostatnie interakcje:\n" + "\n".join([f"    - {action} ({ts[:10]})" for action, ts in interactions])

                dossier = f"""
                Profil: {name}
                Wynik Weryfikacji (Relewancja/AktywnoÅ›Ä‡/Legitymacja): {r}/{a}/{l}
                Podsumowanie Analityka AI: "{summary}"
                {interaction_history}
                """
                dossiers.append(dossier)
            
            # 4. PrzekaÅ¼ wszystko do LLM w celu ostatecznej syntezy
            synthesis_prompt = f"""
            JesteÅ› doradcÄ… strategicznym analizujÄ…cym wydajnoÅ›Ä‡ sieciowÄ… agenta AI. Twoim zadaniem jest zinterpretowanie poniÅ¼szych danych i przedstawienie zwiÄ™zÅ‚ego raportu dla operatora.

            Dane wejÅ›ciowe:
            ---
            {funnel_summary}
            ---
            SzczegÃ³Å‚owe Dossier dla Top 3 PartnerÃ³w:
            {"---".join(dossiers)}
            ---

            Twoje zadanie:
            1.  Przedstaw ogÃ³lny stan lejka partnerskiego.
            2.  Dla kaÅ¼dego z partnerÃ³w z dossier, podsumuj jego profil i dotychczasowe dziaÅ‚ania.
            3.  Na podstawie logiki agenta (ktÃ³ry priorytetyzuje interakcje z wysoko ocenionymi, zweryfikowanymi celami), okreÅ›l, jakie sÄ… prawdopodobne **planowane nastÄ™pne kroki** wobec kaÅ¼dego z nich.
            4.  ZakoÅ„cz jednÄ…, ogÃ³lnÄ… **rekomendacjÄ… strategicznÄ…** (np. "Agent powinien skupiÄ‡ siÄ™ na konwersji zweryfikowanych celÃ³w w aktywne zaangaÅ¼owanie poprzez tworzenie dedykowanych treÅ›ci.").

            Wygeneruj zwiÄ™zÅ‚y raport.
            """
            
            print("ğŸ¤– SyntezujÄ™ raport strategiczny...")
            response = self.openai_client.chat.completions.create(
                model="gpt-4-turbo",
                messages=[{"role": "user", "content": synthesis_prompt}]
            )
            return response.choices[0].message.content

        except Exception as e:
            return f"Krytyczny bÅ‚Ä…d podczas analizy lejka partnerskiego: {e}"

def main():
    try:
        interface = AgentInterface()
        print_help()
        
        while True:
            # ... (reszta pÄ™tli bez zmian)

            command = interface.interpret_command(user_input)
            tool = command.get("tool")
            args = command.get("args", {})
            
            print("-" * 20)
            
            # --- ZAKTUALIZOWANY SÅOWNIK NARZÄ˜DZI ---
            tool_map = {
                "generate_strategy_report": interface.generate_strategy_report,
                "synthesize_answer_from_memory": interface.synthesize_answer_from_memory,
                "add_directive": interface.add_directive,
                "simulate_post_generation": interface.simulate_post_generation,
                "analyze_topic_performance": interface.analyze_topic_performance,
                "list_memory_by_type": interface.list_memory_by_type,
                "list_vetted_partners": interface.list_vetted_partners,
                "analyze_partner_funnel": interface.analyze_partner_funnel
            }

            if tool in tool_map:
                result = tool_map[tool](**args) if args else tool_map[tool]()
                print(result)
            elif tool == "general_conversation":
                print("Jestem interfejsem do zarzÄ…dzania. Skupmy siÄ™ na strategii. Jak mogÄ™ Ci pomÃ³c?")
            else:
                print("Nie udaÅ‚o mi siÄ™ zinterpretowaÄ‡ polecenia. SprÃ³buj sformuÅ‚owaÄ‡ je inaczej lub wpisz 'pomoc'.")
            
            print("-" * 20 + "\n")

    except Exception as e:
        print(f"\nFATALNY BÅÄ„D: Aplikacja zostaÅ‚a zamkniÄ™ta. PowÃ³d: {e}")
    finally:
        print("\nZamykanie poÅ‚Ä…czenia z Centrum Dowodzenia. Do zobaczenia.")

if __name__ == "__main__":
    main()